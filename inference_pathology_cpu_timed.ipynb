{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nlemke/remote/miniconda3/envs/nca3/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/home/nlemke/remote/miniconda3/envs/nca3/lib/python3.10/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.\n",
      "  _torch_pytree._register_pytree_node(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<torch.autograd.grad_mode.set_grad_enabled at 0x7ba2c67a35b0>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from src.datasets.Dataset_PESO import Dataset_PESO\n",
    "from src.utils.Study import Study\n",
    "from src.utils.ProjectConfiguration import ProjectConfiguration\n",
    "from src.utils.BaselineConfigs import EXP_OctreeNCA\n",
    "from src.datasets.Dataset_BCSS_Seg import Dataset_BCSS_Seg\n",
    "from src.datasets.Dataset_AGGC import Dataset_AGGC\n",
    "import octree_vis, torch, os, json, openslide, math\n",
    "import einops\n",
    "from src.models.Model_OctreeNCAV2 import OctreeNCAV2\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "from src.utils.ProjectConfiguration import ProjectConfiguration as pc\n",
    "from src.models.Model_OctreeNCA_2d_patching2 import OctreeNCA2DPatch2\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "torch.set_grad_enabled(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading patches\n",
      "Loaded configuration is missing keys: '['experiment.run_hash']'. Check if you are loading the correct experiment.\n",
      "Reload State 2000\n",
      "loading patches\n",
      "loading patches\n",
      "\n",
      "-------- Experiment Setup --------\n",
      "{\n",
      "    \"experiment.name\": \"peso_med\",\n",
      "    \"experiment.description\": \"OctreeNCA2DSegmentation\",\n",
      "    \"model.output_channels\": 1,\n",
      "    \"model.channel_n\": 16,\n",
      "    \"model.fire_rate\": 0.5,\n",
      "    \"model.kernel_size\": [\n",
      "        3,\n",
      "        7\n",
      "    ],\n",
      "    \"model.hidden_size\": 64,\n",
      "    \"model.batchnorm_track_running_stats\": false,\n",
      "    \"model.train.patch_sizes\": [\n",
      "        null,\n",
      "        null\n",
      "    ],\n",
      "    \"model.train.loss_weighted_patching\": false,\n",
      "    \"model.eval.patch_wise\": false,\n",
      "    \"model.octree.res_and_steps\": [\n",
      "        [\n",
      "            [\n",
      "                320,\n",
      "                320\n",
      "            ],\n",
      "            20\n",
      "        ],\n",
      "        [\n",
      "            [\n",
      "                80,\n",
      "                80\n",
      "            ],\n",
      "            40\n",
      "        ]\n",
      "    ],\n",
      "    \"model.octree.separate_models\": true,\n",
      "    \"model.vitca\": false,\n",
      "    \"model.vitca.depth\": 1,\n",
      "    \"model.vitca.heads\": 4,\n",
      "    \"model.vitca.mlp_dim\": 64,\n",
      "    \"model.vitca.dropout\": 0.0,\n",
      "    \"model.vitca.positional_embedding\": \"vit_handcrafted\",\n",
      "    \"model.vitca.embed_cells\": true,\n",
      "    \"model.vitca.embed_dim\": 128,\n",
      "    \"model.vitca.embed_dropout\": 0.0,\n",
      "    \"trainer.optimizer\": \"torch.optim.Adam\",\n",
      "    \"trainer.optimizer.lr\": 0.0016,\n",
      "    \"trainer.optimizer.betas\": [\n",
      "        0.9,\n",
      "        0.99\n",
      "    ],\n",
      "    \"trainer.lr_scheduler\": \"torch.optim.lr_scheduler.ExponentialLR\",\n",
      "    \"trainer.lr_scheduler.gamma\": 0.9992002799440071,\n",
      "    \"trainer.update_lr_per_epoch\": true,\n",
      "    \"trainer.normalize_gradients\": null,\n",
      "    \"trainer.n_epochs\": 2000,\n",
      "    \"trainer.find_best_model_on\": null,\n",
      "    \"trainer.always_eval_in_last_epochs\": null,\n",
      "    \"trainer.ema\": true,\n",
      "    \"trainer.ema.decay\": 0.99,\n",
      "    \"trainer.ema.update_per\": \"epoch\",\n",
      "    \"experiment.dataset.img_path\": \"PESO/peso_training\",\n",
      "    \"experiment.dataset.label_path\": \"PESO/peso_training\",\n",
      "    \"experiment.dataset.keep_original_scale\": true,\n",
      "    \"experiment.dataset.rescale\": true,\n",
      "    \"experiment.dataset.input_size\": [\n",
      "        320,\n",
      "        320\n",
      "    ],\n",
      "    \"experiment.dataset.img_level\": 1,\n",
      "    \"experiment.dataset.patches_path\": \"clmn1/data/PESO_patches/\",\n",
      "    \"experiment.dataset.seed\": 42,\n",
      "    \"model.input_channels\": 3,\n",
      "    \"trainer.num_steps_per_epoch\": 200,\n",
      "    \"trainer.batch_size\": 3,\n",
      "    \"trainer.batch_duplication\": 1,\n",
      "    \"experiment.task\": \"segmentation\",\n",
      "    \"experiment.task.score\": [\n",
      "        \"src.scores.PatchwiseDiceScore.PatchwiseDiceScore\",\n",
      "        \"src.scores.PatchwiseIoUScore.PatchwiseIoUScore\"\n",
      "    ],\n",
      "    \"trainer.losses\": [\n",
      "        \"src.losses.DiceBCELoss.DiceBCELoss\"\n",
      "    ],\n",
      "    \"trainer.losses.parameters\": [\n",
      "        {}\n",
      "    ],\n",
      "    \"trainer.loss_weights\": [\n",
      "        1.0\n",
      "    ],\n",
      "    \"experiment.data_split\": [\n",
      "        0.7,\n",
      "        0,\n",
      "        0.3\n",
      "    ],\n",
      "    \"experiment.save_interval\": 50,\n",
      "    \"experiment.device\": \"cuda:0\",\n",
      "    \"experiment.logging.also_eval_on_train\": false,\n",
      "    \"experiment.logging.track_gradient_norm\": true,\n",
      "    \"experiment.logging.evaluate_interval\": 2001,\n",
      "    \"performance.compile\": false,\n",
      "    \"performance.data_parallel\": false,\n",
      "    \"performance.num_workers\": 8,\n",
      "    \"performance.unlock_CPU\": true,\n",
      "    \"performance.inplace_operations\": true,\n",
      "    \"trainer.datagen.batchgenerators\": true,\n",
      "    \"trainer.datagen.augmentations\": true,\n",
      "    \"trainer.datagen.difficulty_weighted_sampling\": false,\n",
      "    \"trainer.gradient_accumulation\": false,\n",
      "    \"trainer.train_quality_control\": false,\n",
      "    \"experiment.model_path\": \"clmn1/octree_study_new/Experiments/peso_med_OctreeNCA2DSegmentation\",\n",
      "    \"experiment.git_hash\": \"85c5a5709937205e816b12af5a9c7269384b8aed\",\n",
      "    \"experiment.run_hash\": \"913f3a0ad5a24443ad2bf7d5\"\n",
      "}\n",
      "-------- Experiment Setup --------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model_path = \"/local/scratch/clmn1/octree_study_new/Experiments/pesoLargeGroupNorm_OctreeNCA2DSegmentation\"\n",
    "model_path = \"/local/scratch/clmn1/octree_study_new/Experiments/peso_med_OctreeNCA2DSegmentation/\"\n",
    "\n",
    "\n",
    "with open(os.path.join(model_path, \"config.json\")) as f:\n",
    "    config = json.load(f) \n",
    "\n",
    "exp = EXP_OctreeNCA().createExperiment(config, detail_config={}, \n",
    "                                                      dataset_class=Dataset_PESO, dataset_args={\n",
    "                                                            'patches_path': os.path.join(pc.FILER_BASE_PATH, config['experiment.dataset.patches_path']),\n",
    "                                                            'patch_size': config['experiment.dataset.input_size'],\n",
    "                                                            'path': os.path.join(pc.FILER_BASE_PATH, config['experiment.dataset.img_path']),\n",
    "                                                            'img_level': config['experiment.dataset.img_level']\n",
    "                                                      })\n",
    "\n",
    "model: OctreeNCA2DPatch2 = exp.model\n",
    "assert isinstance(model, OctreeNCA2DPatch2)\n",
    "model = model.eval().cpu()\n",
    "model.device = \"cpu\"\n",
    "for backbone in model.backbone_ncas:\n",
    "    backbone.device = \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_names(x: torch.Tensor):\n",
    "    x.names = [None] * len(x.names)\n",
    "    return x\n",
    "\n",
    "def align_tensor_to(x: torch.Tensor, target: str):\n",
    "    if isinstance(target, tuple):\n",
    "        target_str = ' '.join(target)\n",
    "    elif isinstance(target, str): \n",
    "        if max(map(len, target.split())) != 1:\n",
    "            #targets are like \"BCHW\"\n",
    "            target_str = ' '.join(target)\n",
    "        else:\n",
    "            #targets are like \"B C H W\"\n",
    "            target_str = target\n",
    "            target = target.replace(\" \", \"\")\n",
    "\n",
    "\n",
    "    pattern = f\"{' '.join(x.names)} -> {target_str}\"\n",
    "    x = remove_names(x)\n",
    "    x = einops.rearrange(x, pattern)\n",
    "    x.names = tuple(target)\n",
    "    return x\n",
    "\n",
    "def downscale(x: torch.Tensor, out_size):\n",
    "    x = align_tensor_to(x, \"BCHW\")\n",
    "    remove_names(x)\n",
    "\n",
    "    out = F.interpolate(x, size=out_size)\n",
    "    out.names = ('B', 'C', 'H', 'W')\n",
    "    x.names = ('B', 'C', 'H', 'W')\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_resolutions(x_shape, model):\n",
    "    upscale_factors = []\n",
    "    for i in range(len(model.octree_res)-1):\n",
    "        t = []\n",
    "        for c in range(2):\n",
    "            t.append(model.octree_res[i][c]//model.octree_res[i+1][c])\n",
    "        upscale_factors.append(t)\n",
    "\n",
    "    new_octree_res = [tuple(x_shape)]\n",
    "    for i in range(1, len(model.octree_res)):\n",
    "        downsample_factor = np.array(model.octree_res[i-1]) / np.array(model.octree_res[i])\n",
    "        new_octree_res.append([math.ceil(new_octree_res[i-1][0] / downsample_factor[0]), \n",
    "                                math.ceil(new_octree_res[i-1][1] / downsample_factor[1])])\n",
    "    return new_octree_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "((47484, 105289), (23742, 52644), (11871, 26322), (5935, 13161), (2967, 6580), (1483, 3290), (741, 1645), (370, 822))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1527732/427445862.py:23: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at ../c10/core/TensorImpl.h:1928.)\n",
      "  slide.names = ('B', 'H', 'W', 'C')\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'align_tensor_to' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 25\u001b[0m\n\u001b[1;32m     22\u001b[0m slide \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mfrom_numpy(slide)\u001b[38;5;241m.\u001b[39mfloat()\n\u001b[1;32m     23\u001b[0m slide\u001b[38;5;241m.\u001b[39mnames \u001b[38;5;241m=\u001b[39m (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mB\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mH\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mW\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mC\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 25\u001b[0m slide \u001b[38;5;241m=\u001b[39m \u001b[43malign_tensor_to\u001b[49m(slide, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBHWC\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     26\u001b[0m computed_resolutions \u001b[38;5;241m=\u001b[39m compute_resolutions(slide\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m:\u001b[38;5;241m3\u001b[39m], model)\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28mprint\u001b[39m(computed_resolutions)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'align_tensor_to' is not defined"
     ]
    }
   ],
   "source": [
    "subject = \"14\"\n",
    "pos_x, pos_y = 14400, 24320\n",
    "#pos_x, pos_y = 14400 - 1000, 24320\n",
    "size = (16*161, 16*161)\n",
    "#size = (320, 320)\n",
    "\n",
    "slide = openslide.open_slide(f\"/local/scratch/PESO/peso_training/pds_{subject}_HE.tif\")\n",
    "slide = slide.read_region((int(pos_x * slide.level_downsamples[1]),\n",
    "                           int(pos_y * slide.level_downsamples[1])), 1, size)\n",
    "#slide = slide.read_region((int(pos_x * slide.level_downsamples[1]),\n",
    "#                           int(pos_y * slide.level_downsamples[1])), 1, (16*10, 16*10))\n",
    "slide = np.array(slide)[:,:,0:3]\n",
    "slide_cpu = slide\n",
    "\n",
    "mean = np.array([0.485, 0.456, 0.406])\n",
    "std = np.array([0.229, 0.224, 0.225])\n",
    "slide = slide / 255.0\n",
    "slide = (slide - mean) / std\n",
    "\n",
    "slide = slide[None]\n",
    "slide = torch.from_numpy(slide).float()\n",
    "slide.names = ('B', 'H', 'W', 'C')\n",
    "\n",
    "slide = align_tensor_to(slide, \"BHWC\")\n",
    "computed_resolutions = compute_resolutions(slide.shape[1:3], model)\n",
    "print(computed_resolutions)\n",
    "\n",
    "seed = torch.zeros(1, *computed_resolutions[-1], model.channel_n,\n",
    "                                dtype=torch.float, device=slide.device,\n",
    "                                names=('B', 'H', 'W', 'C'))\n",
    "temp = downscale(slide, computed_resolutions[-1])\n",
    "temp = align_tensor_to(temp, \"BHWC\")\n",
    "remove_names(temp)\n",
    "remove_names(seed)\n",
    "seed[:,:,:,:model.input_channels] = temp\n",
    "#temp.names = ('B', 'H', 'W', 'C')\n",
    "#seed.names = ('B', 'H', 'W', 'C')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index 4 is out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m state \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackbone_ncas\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m]\u001b[49m(seed, steps\u001b[38;5;241m=\u001b[39mmodel\u001b[38;5;241m.\u001b[39minference_steps[\u001b[38;5;241m4\u001b[39m], fire_rate\u001b[38;5;241m=\u001b[39mmodel\u001b[38;5;241m.\u001b[39mfire_rate)\n\u001b[1;32m      3\u001b[0m state \u001b[38;5;241m=\u001b[39m einops\u001b[38;5;241m.\u001b[39mrearrange(state, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mB H W C -> B C H W\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      4\u001b[0m state \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mUpsample(size\u001b[38;5;241m=\u001b[39mcomputed_resolutions[\u001b[38;5;241m3\u001b[39m], mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnearest\u001b[39m\u001b[38;5;124m'\u001b[39m)(state)\n",
      "File \u001b[0;32m~/remote/miniconda3/envs/nca3/lib/python3.10/site-packages/torch/nn/modules/container.py:295\u001b[0m, in \u001b[0;36mModuleList.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m    293\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m(\u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_modules\u001b[38;5;241m.\u001b[39mvalues())[idx])\n\u001b[1;32m    294\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 295\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_modules[\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_abs_string_index\u001b[49m\u001b[43m(\u001b[49m\u001b[43midx\u001b[49m\u001b[43m)\u001b[49m]\n",
      "File \u001b[0;32m~/remote/miniconda3/envs/nca3/lib/python3.10/site-packages/torch/nn/modules/container.py:285\u001b[0m, in \u001b[0;36mModuleList._get_abs_string_index\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m    283\u001b[0m idx \u001b[38;5;241m=\u001b[39m operator\u001b[38;5;241m.\u001b[39mindex(idx)\n\u001b[1;32m    284\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;241m-\u001b[39m\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m idx \u001b[38;5;241m<\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m)):\n\u001b[0;32m--> 285\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mIndexError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mindex \u001b[39m\u001b[38;5;132;01m{\u001b[39;00midx\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m is out of range\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    286\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m idx \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    287\u001b[0m     idx \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m)\n",
      "\u001b[0;31mIndexError\u001b[0m: index 4 is out of range"
     ]
    }
   ],
   "source": [
    "state = model.backbone_ncas[4](seed, steps=model.inference_steps[4], fire_rate=model.fire_rate)\n",
    "\n",
    "state = einops.rearrange(state, \"B H W C -> B C H W\")\n",
    "state = torch.nn.Upsample(size=computed_resolutions[3], mode='nearest')(state)\n",
    "temp = F.interpolate(einops.rearrange(slide, \"B H W C -> B C H W\"), size=computed_resolutions[3])\n",
    "state[0,:model.input_channels,:,:] = temp[0]\n",
    "state = einops.rearrange(state, \"B C H W -> B H W C\")\n",
    "state = model.backbone_ncas[3](state, steps=model.inference_steps[3], fire_rate=model.fire_rate)\n",
    "\n",
    "\n",
    "state = einops.rearrange(state, \"B H W C -> B C H W\")\n",
    "state = torch.nn.Upsample(size=computed_resolutions[2], mode='nearest')(state)\n",
    "temp = F.interpolate(einops.rearrange(slide, \"B H W C -> B C H W\"), size=computed_resolutions[2])\n",
    "state[0,:model.input_channels,:,:] = temp[0]\n",
    "state = einops.rearrange(state, \"B C H W -> B H W C\")\n",
    "state = model.backbone_ncas[2](state, steps=model.inference_steps[3], fire_rate=model.fire_rate)\n",
    "\n",
    "\n",
    "state = einops.rearrange(state, \"B H W C -> B C H W\")\n",
    "state = torch.nn.Upsample(size=computed_resolutions[2], mode='nearest')(state)\n",
    "temp = F.interpolate(einops.rearrange(slide, \"B H W C -> B C H W\"), size=computed_resolutions[2])\n",
    "state[0,:model.input_channels,:,:] = temp[0]\n",
    "state = einops.rearrange(state, \"B C H W -> B H W C\")\n",
    "state = model.backbone_ncas[2](state, steps=model.inference_steps[2], fire_rate=model.fire_rate)\n",
    "\n",
    "\n",
    "state = einops.rearrange(state, \"B H W C -> B C H W\")\n",
    "state = torch.nn.Upsample(size=computed_resolutions[1], mode='nearest')(state)\n",
    "temp = F.interpolate(einops.rearrange(slide, \"B H W C -> B C H W\"), size=computed_resolutions[1])\n",
    "state[0,:model.input_channels,:,:] = temp[0]\n",
    "state = einops.rearrange(state, \"B C H W -> B H W C\")\n",
    "state = model.backbone_ncas[1](state, steps=model.inference_steps[1], fire_rate=model.fire_rate)\n",
    "\n",
    "\n",
    "state = einops.rearrange(state, \"B H W C -> B C H W\")\n",
    "state = torch.nn.Upsample(size=computed_resolutions[0], mode='nearest')(state)\n",
    "temp = F.interpolate(einops.rearrange(slide, \"B H W C -> B C H W\"), size=computed_resolutions[0])\n",
    "state[0,:model.input_channels,:,:] = temp[0]\n",
    "state = einops.rearrange(state, \"B C H W -> B H W C\")\n",
    "state = model.backbone_ncas[0](state, steps=model.inference_steps[0], fire_rate=model.fire_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "state = seed\n",
    "state = model.backbone_ncas[1](state, steps=model.inference_steps[1], fire_rate=model.fire_rate)\n",
    "\n",
    "\n",
    "state = einops.rearrange(state, \"B H W C -> B C H W\")\n",
    "state = torch.nn.Upsample(size=computed_resolutions[0], mode='nearest')(state)\n",
    "temp = F.interpolate(einops.rearrange(slide, \"B H W C -> B C H W\"), size=computed_resolutions[0])\n",
    "state[0,:model.input_channels,:,:] = temp[0]\n",
    "state = einops.rearrange(state, \"B C H W -> B H W C\")\n",
    "state = model.backbone_ncas[0](state, steps=model.inference_steps[0], fire_rate=model.fire_rate)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
