{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[ 1,  2,  3],\n",
       "        [ 4,  5,  6]],\n",
       "\n",
       "       [[ 1,  2,  3],\n",
       "        [ 4,  5,  6]],\n",
       "\n",
       "       [[10, 20, 30],\n",
       "        [40, 50, 60]],\n",
       "\n",
       "       [[ 0,  2,  3],\n",
       "        [ 3,  5,  6]]])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "current_patch = np.array([[[1,2,3], [4,5,6]],\n",
    "                          [[1,2,3], [4,5,6]],\n",
    "                          [[10,20,30], [40,50,60]],\n",
    "                          [[0,2,3], [3,5,6]],\n",
    "                          ])# shape: B, Min, Max\n",
    "scale_factor = np.array([2,2,3])\n",
    "current_patch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[  2,   4,   9],\n",
       "        [  8,  10,  18]],\n",
       "\n",
       "       [[  2,   4,   9],\n",
       "        [  8,  10,  18]],\n",
       "\n",
       "       [[ 20,  40,  90],\n",
       "        [ 80, 100, 180]],\n",
       "\n",
       "       [[  0,   4,   9],\n",
       "        [  6,  10,  18]]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "current_patch * scale_factor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_patch[0] = np.array([[1,2,3], [4,5,6]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3593870/532464939.py:2: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at ../c10/core/TensorImpl.h:1921.)\n",
      "  temp.names = ['B', 'Min', 'Max']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(None, None, None)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp = torch.tensor(current_patch) \n",
    "temp.names = ['B', 'Min', 'Max']\n",
    "temp.names = [None, None, None]\n",
    "temp.permute(1,2,0).shape\n",
    "temp.names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.18.82'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torchio as tio\n",
    "tio.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25 104\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "lengths = []\n",
    "for object in os.listdir('/local/scratch/clmn1/data/DAVIS/JPEGImages/480p'):\n",
    "    lengths.append(len(os.listdir(f'/local/scratch/clmn1/data/DAVIS/JPEGImages/480p/{object}')))\n",
    "print(min(lengths), max(lengths))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "arr = np.array([1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16])\n",
    "arr[0:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(480, 854)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import cv2\n",
    "label = cv2.imread(\"/local/scratch/clmn1/data/cholecseg8k/video01/video01_00080/frame_80_endo_color_mask.png\")\n",
    "label = cv2.cvtColor(label, cv2.COLOR_BGR2RGB)\n",
    "label = np.array(label)\n",
    "mask = label == (0,0,0)\n",
    "mask = np.all(mask, axis=-1)\n",
    "mask.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[127, 127, 127],\n",
       "       [186, 183,  75],\n",
       "       [231,  70, 156],\n",
       "       [255, 114, 114],\n",
       "       [255, 160, 165],\n",
       "       [255, 255, 255]], dtype=uint8)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(np.reshape(label, (-1, 3)), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 2, 3])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array([1,2,3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 3, 3, 5)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = np.zeros((3,3,5))\n",
    "b = np.ones((3,3,5))\n",
    "np.stack((a,b)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nlemke/remote/miniconda3/envs/nca3/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/home/nlemke/remote/miniconda3/envs/nca3/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n"
     ]
    }
   ],
   "source": [
    "import pickle as pkl\n",
    "SPLIT_FILE = r\"/local/scratch/clmn1/octree_study/Experiments/cholec_seg_5_OctreeNCA3D/data_split.dt\"\n",
    "SPLIT = pkl.load(open(SPLIT_FILE, \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['video01_16585',\n",
       " 'video01_28660',\n",
       " 'video01_14859',\n",
       " 'video01_28900',\n",
       " 'video01_16425',\n",
       " 'video01_00240',\n",
       " 'video01_28580',\n",
       " 'video01_28740',\n",
       " 'video01_14939',\n",
       " 'video01_16345',\n",
       " 'video01_00160',\n",
       " 'video01_00080',\n",
       " 'video01_15099',\n",
       " 'video01_28820',\n",
       " 'video01_15019',\n",
       " 'video01_00400',\n",
       " 'video52_00480',\n",
       " 'video52_00320',\n",
       " 'video52_00160',\n",
       " 'video52_00240',\n",
       " 'video52_00400',\n",
       " 'video52_02746',\n",
       " 'video52_00000',\n",
       " 'video52_02826',\n",
       " 'video52_02906',\n",
       " 'video52_00080',\n",
       " 'video12_15750',\n",
       " 'video12_19900',\n",
       " 'video12_19500',\n",
       " 'video12_19740',\n",
       " 'video12_19980',\n",
       " 'video12_19660',\n",
       " 'video12_19580',\n",
       " 'video12_15830',\n",
       " 'video17_01803',\n",
       " 'video17_01563',\n",
       " 'video17_01643',\n",
       " 'video17_01963',\n",
       " 'video27_00240',\n",
       " 'video27_00480',\n",
       " 'video27_00400',\n",
       " 'video27_00640',\n",
       " 'video27_00160']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SPLIT.get_images(\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nlemke/remote/miniconda3/envs/nca3/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:143: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "model = torch.nn.Sequential(\n",
    "    torch.nn.Linear(10, 10),\n",
    "    torch.nn.Linear(10, 10),\n",
    "    torch.nn.Linear(10, 10),\n",
    ")\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(torch.optim.Adam(model.parameters()), 2000)\n",
    "print(scheduler.last_epoch)\n",
    "scheduler.step()\n",
    "print(scheduler.last_epoch)\n",
    "scheduler.step()\n",
    "print(scheduler.last_epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "tuple([320,320,2]) == torch.zeros(320,320,2).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1==1==2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[ 0.0000,  0.0000,  0.0000,  0.0000],\n",
       "          [ 0.8415,  0.8415,  0.8415,  0.8415],\n",
       "          [ 0.9093,  0.9093,  0.9093,  0.9093],\n",
       "          [ 0.1411,  0.1411,  0.1411,  0.1411]],\n",
       "\n",
       "         [[ 1.0000,  1.0000,  1.0000,  1.0000],\n",
       "          [ 0.5403,  0.5403,  0.5403,  0.5403],\n",
       "          [-0.4161, -0.4161, -0.4161, -0.4161],\n",
       "          [-0.9900, -0.9900, -0.9900, -0.9900]]]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch, einops\n",
    "from positional_encodings.torch_encodings import PositionalEncoding1D, PositionalEncoding2D, PositionalEncoding3D\n",
    "\n",
    "e = PositionalEncoding2D(2)(torch.ones(1,  4, 4, 2))\n",
    "einops.rearrange(e, 'b h w c -> b c h w')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.0000,  1.0000,  0.0000,  1.0000],\n",
       "         [ 0.8415,  0.9950,  0.0100,  1.0000],\n",
       "         [ 0.9093,  0.9801,  0.0200,  1.0000],\n",
       "         [ 0.1411,  0.9553,  0.0300,  1.0000],\n",
       "         [-0.7568,  0.9211,  0.0400,  1.0000],\n",
       "         [-0.9589,  0.8776,  0.0500,  1.0000],\n",
       "         [-0.2794,  0.8253,  0.0600,  1.0000],\n",
       "         [ 0.6570,  0.7648,  0.0699,  1.0000],\n",
       "         [ 0.9894,  0.6967,  0.0799,  1.0000],\n",
       "         [ 0.4121,  0.6216,  0.0899,  1.0000],\n",
       "         [-0.5440,  0.5403,  0.0998,  0.9999],\n",
       "         [-1.0000,  0.4536,  0.1098,  0.9999],\n",
       "         [-0.5366,  0.3624,  0.1197,  0.9999],\n",
       "         [ 0.4202,  0.2675,  0.1296,  0.9999],\n",
       "         [ 0.9906,  0.1700,  0.1395,  0.9999],\n",
       "         [ 0.6503,  0.0707,  0.1494,  0.9999]]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from src.utils.vitca_utils import vit_positional_encoding\n",
    "t = torch.zeros(1,4,4,4)\n",
    "t = einops.rearrange(t, 'b c h w -> b (h w) c')\n",
    "\n",
    "t = vit_positional_encoding(t.shape[-2], t.shape[-1])\n",
    "\n",
    "#einops.rearrange(t, 'b (h w) c -> b h w c', h=4, w=4)\n",
    "t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 0.0000,  1.0000,  0.0000,  1.0000],\n",
      "         [ 0.8415,  0.5403,  0.0100,  0.9999],\n",
      "         [ 0.9093, -0.4161,  0.0200,  0.9998],\n",
      "         [ 0.1411, -0.9900,  0.0300,  0.9996],\n",
      "         [-0.7568, -0.6536,  0.0400,  0.9992],\n",
      "         [-0.9589,  0.2837,  0.0500,  0.9988],\n",
      "         [-0.2794,  0.9602,  0.0600,  0.9982],\n",
      "         [ 0.6570,  0.7539,  0.0699,  0.9976],\n",
      "         [ 0.9894, -0.1455,  0.0799,  0.9968],\n",
      "         [ 0.4121, -0.9111,  0.0899,  0.9960],\n",
      "         [-0.5440, -0.8391,  0.0998,  0.9950],\n",
      "         [-1.0000,  0.0044,  0.1098,  0.9940],\n",
      "         [-0.5366,  0.8439,  0.1197,  0.9928],\n",
      "         [ 0.4202,  0.9074,  0.1296,  0.9916],\n",
      "         [ 0.9906,  0.1367,  0.1395,  0.9902],\n",
      "         [ 0.6503, -0.7597,  0.1494,  0.9888]]])\n"
     ]
    }
   ],
   "source": [
    "p_enc_2d = PositionalEncoding1D(4)\n",
    "y = torch.zeros((1,16,4))\n",
    "print(p_enc_2d(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[ 0.0000,  1.0000,  0.0000,  1.0000],\n",
      "          [ 0.0000,  1.0000,  0.8415,  0.5403],\n",
      "          [ 0.0000,  1.0000,  0.9093, -0.4161],\n",
      "          [ 0.0000,  1.0000,  0.1411, -0.9900]],\n",
      "\n",
      "         [[ 0.8415,  0.5403,  0.0000,  1.0000],\n",
      "          [ 0.8415,  0.5403,  0.8415,  0.5403],\n",
      "          [ 0.8415,  0.5403,  0.9093, -0.4161],\n",
      "          [ 0.8415,  0.5403,  0.1411, -0.9900]],\n",
      "\n",
      "         [[ 0.9093, -0.4161,  0.0000,  1.0000],\n",
      "          [ 0.9093, -0.4161,  0.8415,  0.5403],\n",
      "          [ 0.9093, -0.4161,  0.9093, -0.4161],\n",
      "          [ 0.9093, -0.4161,  0.1411, -0.9900]],\n",
      "\n",
      "         [[ 0.1411, -0.9900,  0.0000,  1.0000],\n",
      "          [ 0.1411, -0.9900,  0.8415,  0.5403],\n",
      "          [ 0.1411, -0.9900,  0.9093, -0.4161],\n",
      "          [ 0.1411, -0.9900,  0.1411, -0.9900]]]])\n"
     ]
    }
   ],
   "source": [
    "p_enc_2d = PositionalEncoding2D(4)\n",
    "y = torch.zeros((1,4,4,4))\n",
    "print(p_enc_2d(y)) # (1, 6, 2, 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[0.0000, 0.0000],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.0000, 0.0000]],\n",
       "\n",
       "         [[0.8415, 0.0000],\n",
       "          [0.8415, 0.0000],\n",
       "          [0.8415, 0.0000],\n",
       "          [0.8415, 0.0000]],\n",
       "\n",
       "         [[0.9093, 0.0000],\n",
       "          [0.9093, 0.0000],\n",
       "          [0.9093, 0.0000],\n",
       "          [0.9093, 0.0000]],\n",
       "\n",
       "         [[0.1411, 0.0000],\n",
       "          [0.1411, 0.0000],\n",
       "          [0.1411, 0.0000],\n",
       "          [0.1411, 0.0000]]]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pe = torch.zeros(1,4,4,2)\n",
    "for x in range(4):\n",
    "    for y in range(4):\n",
    "        pe[:,x,y,0] = torch.sin(torch.Tensor([x / 10000 ** (4 * 0 / 2)]))\n",
    "\n",
    "pe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, torch.nn as nn, einops\n",
    "\n",
    "inin = torch.rand(1, 4, 16)\n",
    "inin = torch.cat((inin, -inin), dim=1)\n",
    "out = nn.LayerNorm(16, elementwise_affine=False)(inin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.1110)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inin[0,2].var()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-0.5606)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mine = inin - inin.mean(-1, keepdim=True) / (inin.std(-1, keepdim=True) **0.5 + 1e-5)\n",
    "mine[0,0].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2.6450e-07)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out[0,0].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.06666667, 0.64705882, 0.4745098 ])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import colormaps as cmaps\n",
    "import numpy as np\n",
    "np.array(cmaps.bold[1].colors)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "x = torch.rand(2, 3, 4, 4)\n",
    "\n",
    "with torch.no_grad():\n",
    "    _max = torch.amax(x, dim=tuple(range(1,len(x.shape))), keepdim=True)\n",
    "    _min = torch.amin(x, dim=(1,2,3), keepdim=True)\n",
    "\n",
    "\n",
    "rgb_overflow_loss = (x - torch.clamp(x, _min, _max)).abs().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 2, 3)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tuple(range(1,len(x.shape)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
