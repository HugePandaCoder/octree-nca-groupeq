{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[ 1,  2,  3],\n",
       "        [ 4,  5,  6]],\n",
       "\n",
       "       [[ 1,  2,  3],\n",
       "        [ 4,  5,  6]],\n",
       "\n",
       "       [[10, 20, 30],\n",
       "        [40, 50, 60]],\n",
       "\n",
       "       [[ 0,  2,  3],\n",
       "        [ 3,  5,  6]]])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "current_patch = np.array([[[1,2,3], [4,5,6]],\n",
    "                          [[1,2,3], [4,5,6]],\n",
    "                          [[10,20,30], [40,50,60]],\n",
    "                          [[0,2,3], [3,5,6]],\n",
    "                          ])# shape: B, Min, Max\n",
    "scale_factor = np.array([2,2,3])\n",
    "current_patch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[  2,   4,   9],\n",
       "        [  8,  10,  18]],\n",
       "\n",
       "       [[  2,   4,   9],\n",
       "        [  8,  10,  18]],\n",
       "\n",
       "       [[ 20,  40,  90],\n",
       "        [ 80, 100, 180]],\n",
       "\n",
       "       [[  0,   4,   9],\n",
       "        [  6,  10,  18]]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "current_patch * scale_factor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_patch[0] = np.array([[1,2,3], [4,5,6]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3593870/532464939.py:2: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at ../c10/core/TensorImpl.h:1921.)\n",
      "  temp.names = ['B', 'Min', 'Max']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(None, None, None)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp = torch.tensor(current_patch) \n",
    "temp.names = ['B', 'Min', 'Max']\n",
    "temp.names = [None, None, None]\n",
    "temp.permute(1,2,0).shape\n",
    "temp.names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.18.82'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torchio as tio\n",
    "tio.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25 104\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "lengths = []\n",
    "for object in os.listdir('/local/scratch/clmn1/data/DAVIS/JPEGImages/480p'):\n",
    "    lengths.append(len(os.listdir(f'/local/scratch/clmn1/data/DAVIS/JPEGImages/480p/{object}')))\n",
    "print(min(lengths), max(lengths))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "arr = np.array([1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16])\n",
    "arr[0:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(480, 854)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import cv2\n",
    "label = cv2.imread(\"/local/scratch/clmn1/data/cholecseg8k/video01/video01_00080/frame_80_endo_color_mask.png\")\n",
    "label = cv2.cvtColor(label, cv2.COLOR_BGR2RGB)\n",
    "label = np.array(label)\n",
    "mask = label == (0,0,0)\n",
    "mask = np.all(mask, axis=-1)\n",
    "mask.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[127, 127, 127],\n",
       "       [186, 183,  75],\n",
       "       [231,  70, 156],\n",
       "       [255, 114, 114],\n",
       "       [255, 160, 165],\n",
       "       [255, 255, 255]], dtype=uint8)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(np.reshape(label, (-1, 3)), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 2, 3])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array([1,2,3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 3, 3, 5)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = np.zeros((3,3,5))\n",
    "b = np.ones((3,3,5))\n",
    "np.stack((a,b)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nlemke/remote/miniconda3/envs/nca3/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/home/nlemke/remote/miniconda3/envs/nca3/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n"
     ]
    }
   ],
   "source": [
    "import pickle as pkl\n",
    "SPLIT_FILE = r\"/local/scratch/clmn1/octree_study/Experiments/cholec_seg_5_OctreeNCA3D/data_split.dt\"\n",
    "SPLIT = pkl.load(open(SPLIT_FILE, \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['video01_16585',\n",
       " 'video01_28660',\n",
       " 'video01_14859',\n",
       " 'video01_28900',\n",
       " 'video01_16425',\n",
       " 'video01_00240',\n",
       " 'video01_28580',\n",
       " 'video01_28740',\n",
       " 'video01_14939',\n",
       " 'video01_16345',\n",
       " 'video01_00160',\n",
       " 'video01_00080',\n",
       " 'video01_15099',\n",
       " 'video01_28820',\n",
       " 'video01_15019',\n",
       " 'video01_00400',\n",
       " 'video52_00480',\n",
       " 'video52_00320',\n",
       " 'video52_00160',\n",
       " 'video52_00240',\n",
       " 'video52_00400',\n",
       " 'video52_02746',\n",
       " 'video52_00000',\n",
       " 'video52_02826',\n",
       " 'video52_02906',\n",
       " 'video52_00080',\n",
       " 'video12_15750',\n",
       " 'video12_19900',\n",
       " 'video12_19500',\n",
       " 'video12_19740',\n",
       " 'video12_19980',\n",
       " 'video12_19660',\n",
       " 'video12_19580',\n",
       " 'video12_15830',\n",
       " 'video17_01803',\n",
       " 'video17_01563',\n",
       " 'video17_01643',\n",
       " 'video17_01963',\n",
       " 'video27_00240',\n",
       " 'video27_00480',\n",
       " 'video27_00400',\n",
       " 'video27_00640',\n",
       " 'video27_00160']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SPLIT.get_images(\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nlemke/remote/miniconda3/envs/nca3/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:143: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "model = torch.nn.Sequential(\n",
    "    torch.nn.Linear(10, 10),\n",
    "    torch.nn.Linear(10, 10),\n",
    "    torch.nn.Linear(10, 10),\n",
    ")\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(torch.optim.Adam(model.parameters()), 2000)\n",
    "print(scheduler.last_epoch)\n",
    "scheduler.step()\n",
    "print(scheduler.last_epoch)\n",
    "scheduler.step()\n",
    "print(scheduler.last_epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "tuple([320,320,2]) == torch.zeros(320,320,2).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1==1==2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[ 0.0000,  0.0000,  0.0000,  0.0000],\n",
       "          [ 0.8415,  0.8415,  0.8415,  0.8415],\n",
       "          [ 0.9093,  0.9093,  0.9093,  0.9093],\n",
       "          [ 0.1411,  0.1411,  0.1411,  0.1411]],\n",
       "\n",
       "         [[ 1.0000,  1.0000,  1.0000,  1.0000],\n",
       "          [ 0.5403,  0.5403,  0.5403,  0.5403],\n",
       "          [-0.4161, -0.4161, -0.4161, -0.4161],\n",
       "          [-0.9900, -0.9900, -0.9900, -0.9900]]]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch, einops\n",
    "from positional_encodings.torch_encodings import PositionalEncoding1D, PositionalEncoding2D, PositionalEncoding3D\n",
    "\n",
    "e = PositionalEncoding2D(2)(torch.ones(1,  4, 4, 2))\n",
    "einops.rearrange(e, 'b h w c -> b c h w')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.0000,  1.0000,  0.0000,  1.0000],\n",
       "         [ 0.8415,  0.9950,  0.0100,  1.0000],\n",
       "         [ 0.9093,  0.9801,  0.0200,  1.0000],\n",
       "         [ 0.1411,  0.9553,  0.0300,  1.0000],\n",
       "         [-0.7568,  0.9211,  0.0400,  1.0000],\n",
       "         [-0.9589,  0.8776,  0.0500,  1.0000],\n",
       "         [-0.2794,  0.8253,  0.0600,  1.0000],\n",
       "         [ 0.6570,  0.7648,  0.0699,  1.0000],\n",
       "         [ 0.9894,  0.6967,  0.0799,  1.0000],\n",
       "         [ 0.4121,  0.6216,  0.0899,  1.0000],\n",
       "         [-0.5440,  0.5403,  0.0998,  0.9999],\n",
       "         [-1.0000,  0.4536,  0.1098,  0.9999],\n",
       "         [-0.5366,  0.3624,  0.1197,  0.9999],\n",
       "         [ 0.4202,  0.2675,  0.1296,  0.9999],\n",
       "         [ 0.9906,  0.1700,  0.1395,  0.9999],\n",
       "         [ 0.6503,  0.0707,  0.1494,  0.9999]]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from src.utils.vitca_utils import vit_positional_encoding\n",
    "t = torch.zeros(1,4,4,4)\n",
    "t = einops.rearrange(t, 'b c h w -> b (h w) c')\n",
    "\n",
    "t = vit_positional_encoding(t.shape[-2], t.shape[-1])\n",
    "\n",
    "#einops.rearrange(t, 'b (h w) c -> b h w c', h=4, w=4)\n",
    "t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 0.0000,  1.0000,  0.0000,  1.0000],\n",
      "         [ 0.8415,  0.5403,  0.0100,  0.9999],\n",
      "         [ 0.9093, -0.4161,  0.0200,  0.9998],\n",
      "         [ 0.1411, -0.9900,  0.0300,  0.9996],\n",
      "         [-0.7568, -0.6536,  0.0400,  0.9992],\n",
      "         [-0.9589,  0.2837,  0.0500,  0.9988],\n",
      "         [-0.2794,  0.9602,  0.0600,  0.9982],\n",
      "         [ 0.6570,  0.7539,  0.0699,  0.9976],\n",
      "         [ 0.9894, -0.1455,  0.0799,  0.9968],\n",
      "         [ 0.4121, -0.9111,  0.0899,  0.9960],\n",
      "         [-0.5440, -0.8391,  0.0998,  0.9950],\n",
      "         [-1.0000,  0.0044,  0.1098,  0.9940],\n",
      "         [-0.5366,  0.8439,  0.1197,  0.9928],\n",
      "         [ 0.4202,  0.9074,  0.1296,  0.9916],\n",
      "         [ 0.9906,  0.1367,  0.1395,  0.9902],\n",
      "         [ 0.6503, -0.7597,  0.1494,  0.9888]]])\n"
     ]
    }
   ],
   "source": [
    "p_enc_2d = PositionalEncoding1D(4)\n",
    "y = torch.zeros((1,16,4))\n",
    "print(p_enc_2d(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[ 0.0000,  1.0000,  0.0000,  1.0000],\n",
      "          [ 0.0000,  1.0000,  0.8415,  0.5403],\n",
      "          [ 0.0000,  1.0000,  0.9093, -0.4161],\n",
      "          [ 0.0000,  1.0000,  0.1411, -0.9900]],\n",
      "\n",
      "         [[ 0.8415,  0.5403,  0.0000,  1.0000],\n",
      "          [ 0.8415,  0.5403,  0.8415,  0.5403],\n",
      "          [ 0.8415,  0.5403,  0.9093, -0.4161],\n",
      "          [ 0.8415,  0.5403,  0.1411, -0.9900]],\n",
      "\n",
      "         [[ 0.9093, -0.4161,  0.0000,  1.0000],\n",
      "          [ 0.9093, -0.4161,  0.8415,  0.5403],\n",
      "          [ 0.9093, -0.4161,  0.9093, -0.4161],\n",
      "          [ 0.9093, -0.4161,  0.1411, -0.9900]],\n",
      "\n",
      "         [[ 0.1411, -0.9900,  0.0000,  1.0000],\n",
      "          [ 0.1411, -0.9900,  0.8415,  0.5403],\n",
      "          [ 0.1411, -0.9900,  0.9093, -0.4161],\n",
      "          [ 0.1411, -0.9900,  0.1411, -0.9900]]]])\n"
     ]
    }
   ],
   "source": [
    "p_enc_2d = PositionalEncoding2D(4)\n",
    "y = torch.zeros((1,4,4,4))\n",
    "print(p_enc_2d(y)) # (1, 6, 2, 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[0.0000, 0.0000],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.0000, 0.0000]],\n",
       "\n",
       "         [[0.8415, 0.0000],\n",
       "          [0.8415, 0.0000],\n",
       "          [0.8415, 0.0000],\n",
       "          [0.8415, 0.0000]],\n",
       "\n",
       "         [[0.9093, 0.0000],\n",
       "          [0.9093, 0.0000],\n",
       "          [0.9093, 0.0000],\n",
       "          [0.9093, 0.0000]],\n",
       "\n",
       "         [[0.1411, 0.0000],\n",
       "          [0.1411, 0.0000],\n",
       "          [0.1411, 0.0000],\n",
       "          [0.1411, 0.0000]]]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pe = torch.zeros(1,4,4,2)\n",
    "for x in range(4):\n",
    "    for y in range(4):\n",
    "        pe[:,x,y,0] = torch.sin(torch.Tensor([x / 10000 ** (4 * 0 / 2)]))\n",
    "\n",
    "pe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, torch.nn as nn, einops\n",
    "\n",
    "inin = torch.rand(1, 4, 16)\n",
    "inin = torch.cat((inin, -inin), dim=1)\n",
    "out = nn.LayerNorm(16, elementwise_affine=False)(inin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.1110)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inin[0,2].var()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-0.5606)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mine = inin - inin.mean(-1, keepdim=True) / (inin.std(-1, keepdim=True) **0.5 + 1e-5)\n",
    "mine[0,0].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2.6450e-07)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out[0,0].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.06666667, 0.64705882, 0.4745098 ])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import colormaps as cmaps\n",
    "import numpy as np\n",
    "np.array(cmaps.bold[1].colors)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "x = torch.rand(2, 3, 4, 4)\n",
    "\n",
    "with torch.no_grad():\n",
    "    _max = torch.amax(x, dim=tuple(range(1,len(x.shape))), keepdim=True)\n",
    "    _min = torch.amin(x, dim=(1,2,3), keepdim=True)\n",
    "\n",
    "\n",
    "rgb_overflow_loss = (x - torch.clamp(x, _min, _max)).abs().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 2, 3)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tuple(range(1,len(x.shape)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchio as tio\n",
    "import torch\n",
    "severity = 2\n",
    "x = torch.randn(1, 32, 32, 16)\n",
    "#ghosting\n",
    "transform = tio.RandomGhosting(num_ghosts=severity, intensity=0.25 * severity)\n",
    "\n",
    "x = transform(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandomGhosting()\n"
     ]
    }
   ],
   "source": [
    "print(transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7e73d06ebe50>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.subplot(1,2,1)\n",
    "plt.imshow(np.zeros((32,32)), vmin=0, vmax=1)\n",
    "plt.subplot(1,2,2)\n",
    "plt.imshow(np.ones((32,32)), vmin=0, vmax=1)\n",
    "\n",
    "#plt.colorbar()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
