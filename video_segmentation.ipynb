{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nlemke/remote/miniconda3/envs/nca3/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/home/nlemke/remote/miniconda3/envs/nca3/lib/python3.10/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.\n",
      "  _torch_pytree._register_pytree_node(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<torch.autograd.grad_mode.set_grad_enabled at 0x7fb7c41bf3d0>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import imageio, json, os, torch, einops, math, tqdm\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "\n",
    "from src.datasets.Dataset_CholecSeg_preprocessed import Dataset_CholecSeg_preprocessed\n",
    "from src.utils.BaselineConfigs import EXP_OctreeNCA3D\n",
    "\n",
    "torch.set_grad_enabled(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_path = \"/local/scratch/Cholec80/cholec80_full_set/videos/video01.mp4\"\n",
    "video_reader = imageio.get_reader(video_path)\n",
    "n_frames = video_reader.get_meta_data()['duration'] * video_reader.get_meta_data()['fps']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1500, 240, 424, 3)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_seconds = 60\n",
    "\n",
    "video = []\n",
    "for frame in range(int(video_reader.get_meta_data()['fps'] * num_seconds)):\n",
    "    image = video_reader.get_data(frame)\n",
    "    video.append(image[None, ...])\n",
    "\n",
    "video = np.concatenate(video, axis=0)\n",
    "video = einops.rearrange(video, 't h w c ->  h w (t c)')\n",
    "\n",
    "\n",
    "outstacks = []\n",
    "for i in range(math.ceil(video.shape[-1] / 500)):\n",
    "    outstack = cv2.resize(video[..., i*500:(i+1)*500], (424, 240))\n",
    "    outstacks.append(outstack)\n",
    "video = np.concatenate(outstacks, axis=-1)\n",
    "video = einops.rearrange(video, 'h w (t c) -> t h w c', c=3).astype(np.float32)\n",
    "video /= 255.0\n",
    "mean = [0.485, 0.456, 0.406]\n",
    "std = [0.229, 0.224, 0.225]\n",
    "video -= mean\n",
    "video /= std\n",
    "video.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded configuration is missing keys: '['experiment.run_hash']'. Check if you are loading the correct experiment.\n",
      "Reload State 2000\n",
      "\n",
      "-------- Experiment Setup --------\n",
      "{\n",
      "    \"experiment.name\": \"cholecfAbl_none_10_1.0_16_3_1.0_0.99\",\n",
      "    \"experiment.description\": \"OctreeNCASegmentation\",\n",
      "    \"model.output_channels\": 5,\n",
      "    \"model.channel_n\": 16,\n",
      "    \"model.fire_rate\": 0.5,\n",
      "    \"model.kernel_size\": [\n",
      "        3,\n",
      "        3,\n",
      "        3,\n",
      "        3,\n",
      "        3\n",
      "    ],\n",
      "    \"model.hidden_size\": 64,\n",
      "    \"model.batchnorm_track_running_stats\": false,\n",
      "    \"model.train.patch_sizes\": [\n",
      "        [\n",
      "            60,\n",
      "            106,\n",
      "            20\n",
      "        ],\n",
      "        [\n",
      "            60,\n",
      "            106,\n",
      "            20\n",
      "        ],\n",
      "        null,\n",
      "        null,\n",
      "        null\n",
      "    ],\n",
      "    \"model.train.loss_weighted_patching\": false,\n",
      "    \"model.eval.patch_wise\": false,\n",
      "    \"model.octree.res_and_steps\": [\n",
      "        [\n",
      "            [\n",
      "                240,\n",
      "                432,\n",
      "                80\n",
      "            ],\n",
      "            10\n",
      "        ],\n",
      "        [\n",
      "            [\n",
      "                120,\n",
      "                216,\n",
      "                40\n",
      "            ],\n",
      "            10\n",
      "        ],\n",
      "        [\n",
      "            [\n",
      "                60,\n",
      "                108,\n",
      "                20\n",
      "            ],\n",
      "            10\n",
      "        ],\n",
      "        [\n",
      "            [\n",
      "                30,\n",
      "                54,\n",
      "                10\n",
      "            ],\n",
      "            10\n",
      "        ],\n",
      "        [\n",
      "            [\n",
      "                15,\n",
      "                27,\n",
      "                5\n",
      "            ],\n",
      "            27\n",
      "        ]\n",
      "    ],\n",
      "    \"model.octree.separate_models\": true,\n",
      "    \"model.backbone_class\": \"BasicNCA3DFast\",\n",
      "    \"model.vitca\": false,\n",
      "    \"model.vitca.depth\": 1,\n",
      "    \"model.vitca.heads\": 4,\n",
      "    \"model.vitca.mlp_dim\": 64,\n",
      "    \"model.vitca.dropout\": 0.0,\n",
      "    \"model.vitca.positional_embedding\": \"vit_handcrafted\",\n",
      "    \"model.vitca.embed_cells\": true,\n",
      "    \"model.vitca.embed_dim\": 128,\n",
      "    \"model.vitca.embed_dropout\": 0.0,\n",
      "    \"trainer.optimizer\": \"torch.optim.Adam\",\n",
      "    \"trainer.optimizer.lr\": 0.0016,\n",
      "    \"trainer.optimizer.betas\": [\n",
      "        0.9,\n",
      "        0.99\n",
      "    ],\n",
      "    \"trainer.lr_scheduler\": \"torch.optim.lr_scheduler.ExponentialLR\",\n",
      "    \"trainer.lr_scheduler.gamma\": 0.9992002799440071,\n",
      "    \"trainer.update_lr_per_epoch\": true,\n",
      "    \"trainer.normalize_gradients\": null,\n",
      "    \"trainer.n_epochs\": 2000,\n",
      "    \"trainer.find_best_model_on\": null,\n",
      "    \"trainer.always_eval_in_last_epochs\": null,\n",
      "    \"trainer.ema\": true,\n",
      "    \"trainer.ema.decay\": 0.99,\n",
      "    \"trainer.ema.update_per\": \"epoch\",\n",
      "    \"experiment.dataset.img_path\": \"clmn1/data/cholecseg8k_preprocessed_2/\",\n",
      "    \"experiment.dataset.label_path\": \"clmn1/data/cholecseg8k_preprocessed_2/\",\n",
      "    \"experiment.dataset.keep_original_scale\": true,\n",
      "    \"experiment.dataset.rescale\": true,\n",
      "    \"experiment.dataset.input_size\": [\n",
      "        240,\n",
      "        432,\n",
      "        80\n",
      "    ],\n",
      "    \"experiment.dataset.split_file\": \"clmn1/octree_study/cholec_split.pkl\",\n",
      "    \"model.input_channels\": 3,\n",
      "    \"trainer.num_steps_per_epoch\": null,\n",
      "    \"trainer.batch_size\": 3,\n",
      "    \"trainer.batch_duplication\": 1,\n",
      "    \"experiment.task\": \"segmentation\",\n",
      "    \"experiment.task.score\": [\n",
      "        \"src.scores.DiceScore.DiceScore\",\n",
      "        \"src.scores.IoUScore.IoUScore\"\n",
      "    ],\n",
      "    \"trainer.losses\": [\n",
      "        \"src.losses.DiceLoss.DiceLoss\",\n",
      "        \"src.losses.BCELoss.BCELoss\"\n",
      "    ],\n",
      "    \"trainer.losses.parameters\": [\n",
      "        {},\n",
      "        {}\n",
      "    ],\n",
      "    \"trainer.loss_weights\": [\n",
      "        1.0,\n",
      "        1.0\n",
      "    ],\n",
      "    \"experiment.data_split\": [\n",
      "        0.7,\n",
      "        0,\n",
      "        0.3\n",
      "    ],\n",
      "    \"experiment.save_interval\": 50,\n",
      "    \"experiment.device\": \"cuda:0\",\n",
      "    \"experiment.logging.also_eval_on_train\": false,\n",
      "    \"experiment.logging.track_gradient_norm\": true,\n",
      "    \"experiment.logging.evaluate_interval\": 50,\n",
      "    \"performance.compile\": false,\n",
      "    \"performance.data_parallel\": false,\n",
      "    \"performance.num_workers\": 8,\n",
      "    \"performance.unlock_CPU\": true,\n",
      "    \"performance.inplace_operations\": true,\n",
      "    \"trainer.datagen.batchgenerators\": true,\n",
      "    \"trainer.datagen.augmentations\": true,\n",
      "    \"trainer.datagen.difficulty_weighted_sampling\": false,\n",
      "    \"trainer.gradient_accumulation\": false,\n",
      "    \"trainer.train_quality_control\": false,\n",
      "    \"model.normalization\": \"none\",\n",
      "    \"experiment.model_path\": \"clmn1/octree_study_new/Experiments/cholecfAbl_none_10_1.0_16_3_1.0_0.99_OctreeNCASegmentation\",\n",
      "    \"experiment.git_hash\": \"0cc72993c85b2255f975996acacda676e8519749\",\n",
      "    \"experiment.run_hash\": \"8054a345fd2e42b9ba16a809\"\n",
      "}\n",
      "-------- Experiment Setup --------\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "OctreeNCA3DPatch2(\n",
       "  (backbone_ncas): ModuleList(\n",
       "    (0-4): 5 x BasicNCA3DFast(\n",
       "      (fc0): Conv3d(32, 64, kernel_size=(1, 1, 1), stride=(1, 1, 1))\n",
       "      (fc1): Conv3d(64, 13, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
       "      (conv): Conv3d(16, 16, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), groups=16, padding_mode=reflect)\n",
       "      (bn): Identity()\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from src.models.Model_OctreeNCA_3d_patching2 import OctreeNCA3DPatch2\n",
    "\n",
    "\n",
    "model_path = \"/local/scratch/clmn1/octree_study/Experiments/cholec_seg_octree_2_OctreeNCA3D\"\n",
    "model_path = \"/local/scratch/clmn1/octree_study_new/Experiments/cholecfAbl_none_10_1.0_16_3_1.0_0.99_OctreeNCASegmentation/\"\n",
    "with open(os.path.join(model_path, \"config.json\")) as f:\n",
    "    config = json.load(f) \n",
    "\n",
    "exp = EXP_OctreeNCA3D().createExperiment(config, detail_config={}, \n",
    "                                        dataset_class=Dataset_CholecSeg_preprocessed, dataset_args = {\n",
    "                                        })\n",
    "\n",
    "model: OctreeNCA3DPatch2 = exp.model\n",
    "assert isinstance(model, OctreeNCA3DPatch2)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "for backbone in model.backbone_ncas:\n",
    "    backbone.use_forward_cuda = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def downscale(x: torch.Tensor, out_size):\n",
    "    x = model.align_tensor_to(x, \"BCHWD\")\n",
    "    model.remove_names(x)\n",
    "\n",
    "    out = F.interpolate(x, size=out_size)\n",
    "    out.names = ('B', 'C', 'H', 'W', 'D')\n",
    "    x.names = ('B', 'C', 'H', 'W', 'D')\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[240, 424, 1500], [120, 212, 750], [60, 106, 375], [30, 53, 188], [15, 27, 94]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3341814/3070753857.py:2: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at ../c10/core/TensorImpl.h:1928.)\n",
      "  video_tensor.names = ('B', 'H', 'W', 'D', 'C')\n"
     ]
    }
   ],
   "source": [
    "video_tensor = torch.from_numpy(einops.rearrange(video, 'D H W C -> 1 H W D C'))\n",
    "video_tensor.names = ('B', 'H', 'W', 'D', 'C')\n",
    "computed_resolutions = model.compute_octree_res(video_tensor)\n",
    "print(computed_resolutions)\n",
    "\n",
    "seed = torch.zeros(1, *computed_resolutions[-1], model.channel_n,\n",
    "                                dtype=torch.float, device=model.device, \n",
    "                                names=('B', 'H', 'W', 'D', 'C'))\n",
    "temp = downscale(video_tensor, computed_resolutions[-1])\n",
    "temp = model.align_tensor_to(temp, \"BHWDC\")\n",
    "model.remove_names(temp)\n",
    "model.remove_names(seed)\n",
    "seed[:,:,:,:,:model.input_channels] = temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 15, 27, 94, 16])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seed.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[10, 10, 10, 10, 27]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.inference_steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3d CUDA quick!\n",
      "3d CUDA quick!\n",
      "3d CUDA quick!\n",
      "3d CUDA quick!\n",
      "3d CUDA quick!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/8 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3d CUDA quick!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▎        | 1/8 [00:01<00:07,  1.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3d CUDA quick!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██▌       | 2/8 [00:02<00:06,  1.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3d CUDA quick!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 38%|███▊      | 3/8 [00:03<00:05,  1.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3d CUDA quick!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 4/8 [00:04<00:04,  1.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3d CUDA quick!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 62%|██████▎   | 5/8 [00:05<00:03,  1.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3d CUDA quick!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|███████▌  | 6/8 [00:06<00:02,  1.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3d CUDA quick!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 88%|████████▊ | 7/8 [00:07<00:01,  1.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3d CUDA quick!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8/8 [00:08<00:00,  1.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken:  9.849893808364868\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "state = model.backbone_ncas[3](seed, steps=model.inference_steps[3], fire_rate=model.fire_rate)\n",
    "\n",
    "state = einops.rearrange(state, '1 H W D C -> 1 C H W D')\n",
    "state = torch.nn.Upsample(size=computed_resolutions[2], mode='nearest')(state)\n",
    "temp = F.interpolate(einops.rearrange(video_tensor, \"1 h w t c -> 1 c h w t\"), size=computed_resolutions[2])\n",
    "state[0,:model.input_channels,:,:,:] = temp[0]\n",
    "state = einops.rearrange(state, '1 C H W D -> 1 H W D C')\n",
    "state = model.backbone_ncas[2](state, steps=model.inference_steps[2], fire_rate=model.fire_rate)\n",
    "\n",
    "state = einops.rearrange(state, '1 H W D C -> 1 C H W D')\n",
    "state = torch.nn.Upsample(size=computed_resolutions[1], mode='nearest')(state)\n",
    "temp = F.interpolate(einops.rearrange(video_tensor, \"1 h w t c -> 1 c h w t\"), size=computed_resolutions[1])\n",
    "state[0,:model.input_channels,:,:,:] = temp[0]\n",
    "state = einops.rearrange(state, '1 C H W D -> 1 H W D C')\n",
    "\n",
    "new_state = torch.zeros_like(state)\n",
    "PATCH_SIZE = 300\n",
    "PADDING = model.inference_steps[1]\n",
    "for i in range(0, state.shape[3], PATCH_SIZE):\n",
    "    write_start_idx = max(i, 0)\n",
    "    write_end_idx = min(i+PATCH_SIZE, state.shape[3])\n",
    "    load_start_idx = max(i-PADDING, 0)\n",
    "    load_end_idx = min(i+PATCH_SIZE+PADDING, state.shape[3])\n",
    "    #write to new_state[:,:,:,write_start_idx:write_end_idx,:]\n",
    "    #load from state[:,:,:,load_start_idx:load_end_idx,:]\n",
    "\n",
    "    padding_start = write_start_idx - load_start_idx\n",
    "    padding_end = load_end_idx - write_end_idx\n",
    "    #print(f\"[{write_start_idx}, {write_end_idx}], [{load_start_idx}, {load_end_idx}], ({padding_start}, {padding_end})\")\n",
    "    temp_state = state[:,:,:,load_start_idx:load_end_idx,:]\n",
    "    temp = model.backbone_ncas[1](temp_state, steps=model.inference_steps[1], fire_rate=model.fire_rate)\n",
    "    #temp = torch.zeros_like(temp_state)\n",
    "\n",
    "\n",
    "    new_state[:,:,:,write_start_idx:write_end_idx,:] = temp[:,:,:,padding_start:temp.shape[3]-padding_end,:]\n",
    "\n",
    "state = new_state\n",
    "\n",
    "\n",
    "state = einops.rearrange(state, '1 H W D C -> 1 C H W D').cpu()\n",
    "state = torch.nn.Upsample(size=computed_resolutions[0], mode='nearest')(state)\n",
    "temp = F.interpolate(einops.rearrange(video_tensor, \"1 h w t c -> 1 c h w t\"), size=computed_resolutions[0])\n",
    "state[0,:model.input_channels,:,:,:] = temp[0]\n",
    "state = einops.rearrange(state, '1 C H W D -> 1 H W D C')\n",
    "\n",
    "del video_tensor, video_reader, video\n",
    "\n",
    "new_state = torch.zeros(1, *computed_resolutions[0], model.output_channels, names=('B', 'H', 'W', 'D', 'C'))\n",
    "\n",
    "PATCH_SIZE = 200\n",
    "PADDING = model.inference_steps[0]\n",
    "for i in tqdm.tqdm(range(0, state.shape[3], PATCH_SIZE)):\n",
    "    write_start_idx = max(i, 0)\n",
    "    write_end_idx = min(i+PATCH_SIZE, state.shape[3])\n",
    "    load_start_idx = max(i-PADDING, 0)\n",
    "    load_end_idx = min(i+PATCH_SIZE+PADDING, state.shape[3])\n",
    "    #write to new_state[:,:,:,write_start_idx:write_end_idx,:]\n",
    "    #load from state[:,:,:,load_start_idx:load_end_idx,:]\n",
    "\n",
    "    padding_start = write_start_idx - load_start_idx\n",
    "    padding_end = load_end_idx - write_end_idx\n",
    "    #print(f\"[{write_start_idx}, {write_end_idx}], [{load_start_idx}, {load_end_idx}], ({padding_start}, {padding_end})\")\n",
    "    temp_state = state[:,:,:,load_start_idx:load_end_idx,:]\n",
    "    temp = model.backbone_ncas[0](temp_state.to(model.device), steps=model.inference_steps[1], fire_rate=model.fire_rate).cpu()\n",
    "    #temp = torch.zeros_like(temp_state)\n",
    "\n",
    "\n",
    "    new_state[:,:,:,write_start_idx:write_end_idx,:] = temp[:,:,:,padding_start:temp.shape[3]-padding_end,model.input_channels:model.input_channels+model.output_channels]\n",
    "\n",
    "state = new_state\n",
    "\n",
    "end = time.time()\n",
    "print(\"Time taken: \", end-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 240, 424, 1500, 5])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state.shape # B H W D C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "segmentation = (state > 0).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "color_dict={\n",
    "    0: (252, 111, 3), \n",
    "    1: (252, 3, 227), \n",
    "    2: (205, 214, 34),\n",
    "    3: (150, 150, 150),\n",
    "    4: (0, 173, 29),\n",
    "}\n",
    "video_reader = imageio.get_reader(video_path)\n",
    "video = []\n",
    "for frame in range(int(video_reader.get_meta_data()['fps'] * num_seconds)):\n",
    "    image = video_reader.get_data(frame)\n",
    "    video.append(image[None, ...])\n",
    "\n",
    "video = np.concatenate(video, axis=0)\n",
    "video = einops.rearrange(video, 't h w c ->  h w (t c)')\n",
    "\n",
    "\n",
    "outstacks = []\n",
    "for i in range(math.ceil(video.shape[-1] / 500)):\n",
    "    outstack = cv2.resize(video[..., i*500:(i+1)*500], (424, 240))\n",
    "    outstacks.append(outstack)\n",
    "video = np.concatenate(outstacks, axis=-1)\n",
    "video = einops.rearrange(video, 'h w (t c) -> t h w c', c=3).astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 240, 424, 1500, 5)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "segmentation.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.0140)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from src.scores.TemporalConsistency import mean_temporal_flicker\n",
    "mean_temporal_flicker(torch.from_numpy(einops.rearrange(segmentation, \"b h w t c -> b c h w t\")).float())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "out = cv2.VideoWriter('output_0.mp4', cv2.VideoWriter_fourcc(*'mp4v'), 25, (424, 240), True)\n",
    "for i in range(segmentation.shape[3]):\n",
    "    frame_seg = segmentation[0, :, :, i]\n",
    "    frame = np.zeros((segmentation.shape[1], segmentation.shape[2], 3), dtype=np.uint8)\n",
    "    for c in range(frame.shape[-1]):\n",
    "        frame[frame_seg[..., c], :] = color_dict[c]\n",
    "    frame = 0.2 * frame + 0.8 * video[i]\n",
    "    frame = frame.astype(np.uint8)\n",
    "    out.write(frame)\n",
    "\n",
    "out.release()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
