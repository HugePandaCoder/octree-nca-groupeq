{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nlemke/remote/miniconda3/envs/nca3/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/home/nlemke/remote/miniconda3/envs/nca3/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<torch.autograd.grad_mode.set_grad_enabled at 0x7ca0682d2fe0>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import imageio, json, os, torch, einops, math, tqdm\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from src.datasets.Dataset_CholecSeg_preprocessed import Dataset_CholecSeg_preprocessed\n",
    "from src.utils.BaselineConfigs import EXP_OctreeNCA3D\n",
    "\n",
    "torch.set_grad_enabled(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_path = \"/local/scratch/Cholec80/cholec80_full_set/videos/video01.mp4\"\n",
    "video_reader = imageio.get_reader(video_path)\n",
    "n_frames = video_reader.get_meta_data()['duration'] * video_reader.get_meta_data()['fps']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1500, 240, 424, 3)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_seconds = 60\n",
    "\n",
    "video = []\n",
    "for frame in range(int(video_reader.get_meta_data()['fps'] * num_seconds)):\n",
    "    image = video_reader.get_data(frame)\n",
    "    video.append(image[None, ...])\n",
    "\n",
    "video = np.concatenate(video, axis=0)\n",
    "video = einops.rearrange(video, 't h w c ->  h w (t c)')\n",
    "\n",
    "\n",
    "outstacks = []\n",
    "for i in range(math.ceil(video.shape[-1] / 500)):\n",
    "    outstack = cv2.resize(video[..., i*500:(i+1)*500], (424, 240))\n",
    "    outstacks.append(outstack)\n",
    "video = np.concatenate(outstacks, axis=-1)\n",
    "video = einops.rearrange(video, 'h w (t c) -> t h w c', c=3).astype(np.float32)\n",
    "video /= 255.0\n",
    "mean = [0.485, 0.456, 0.406]\n",
    "std = [0.229, 0.224, 0.225]\n",
    "video -= mean\n",
    "video /= std\n",
    "video.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CONFIG {'description': 'OctreeNCA3D', 'batch_duplication': 1, 'channel_n': 16, 'inference_steps': 64, 'cell_fire_rate': 0.5, 'batch_size': 2, 'hidden_size': 64, 'train_model': 1, 'kernel_size': [3, 3, 3, 7], 'scale_factor': 4, 'levels': 2, 'input_size': [[240, 424, 80]], 'batchnorm_track_running_stats': False, 'gradient_accumulation': False, 'track_gradient_norm': True, 'train_quality_control': False, 'apply_ema': False, 'ema_decay': 0.999, 'ema_update_per': 'epoch', 'find_best_model_on': None, 'always_eval_in_last_epochs': None, 'optimizer': 'Adam', 'sgd_momentum': 0.99, 'sgd_nesterov': True, 'betas': [0.9, 0.99], 'scheduler': 'exponential', 'lr': 0.0016, 'polynomial_scheduler_power': 1.8, 'lr_gamma': 0.9992002799440071, 'num_workers': 8, 'batchgenerators': True, 'img_path': '/local/scratch/clmn1/data/cholecseg8k_preprocessed_2/', 'label_path': '/local/scratch/clmn1/data/cholecseg8k_preprocessed_2/', 'name': 'cholec_seg_octree_2', 'device': 'cuda:0', 'unlock_CPU': True, 'save_interval': 50, 'evaluate_interval': 2001, 'n_epoch': 2000, 'input_channels': 3, 'output_channels': 5, 'data_split': [0.7, 0, 0.3], 'keep_original_scale': True, 'rescale': True, 'octree_res_and_steps': [[[240, 424, 80], 20], [[120, 212, 40], 20], [[60, 106, 20], 20], [[30, 53, 10], 40]], 'separate_models': True, 'patch_sizes': [[60, 106, 20], [60, 106, 20], None, None], 'compile': True, 'data_parallel': False, 'update_lr_per_epoch': True, 'also_eval_on_train': True, 'num_steps_per_epoch': None, 'train_data_augmentations': True, 'loss_weighted_patching': False, 'difficulty_weighted_sampling': False, 'Persistence': False, 'model_path': '/local/scratch/clmn1/octree_study/Experiments/cholec_seg_octree_2_OctreeNCA3D', 'generate_path': '/local/scratch/clmn1/octree_study/Experiments/cholec_seg_octree_2/Generated', 'git_hash': 'a617bea8bf1b623577a80ab0d245930e05b32828', 'hash': '7f16b2a448a34de99c59f461'}\n",
      "/local/scratch/clmn1/octree_study/Experiments/cholec_seg_octree_2_OctreeNCA3D/data_split.dt\n",
      "7f16b2a448a34de99c59f461\n",
      "/local/scratch/clmn1/octree_study/Experiments/cholec_seg_octree_2_OctreeNCA3D/models/epoch_2000\n",
      "Reload State 2000\n",
      "DATA INSTANCE CREATED\n",
      "DATA INSTANCE CREATED\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "OctreeNCA3DPatch2(\n",
       "  (backbone_ncas): ModuleList(\n",
       "    (0-2): 3 x OptimizedModule(\n",
       "      (_orig_mod): BasicNCA3D(\n",
       "        (fc0): Linear(in_features=32, out_features=64, bias=True)\n",
       "        (fc1): Linear(in_features=64, out_features=16, bias=False)\n",
       "        (p0): Conv3d(16, 16, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), groups=16, padding_mode=reflect)\n",
       "        (bn): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
       "      )\n",
       "    )\n",
       "    (3): OptimizedModule(\n",
       "      (_orig_mod): BasicNCA3D(\n",
       "        (fc0): Linear(in_features=32, out_features=64, bias=True)\n",
       "        (fc1): Linear(in_features=64, out_features=16, bias=False)\n",
       "        (p0): Conv3d(16, 16, kernel_size=(7, 7, 7), stride=(1, 1, 1), padding=(3, 3, 3), groups=16, padding_mode=reflect)\n",
       "        (bn): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from src.models.Model_OctreeNCA_3d_patching2 import OctreeNCA3DPatch2\n",
    "\n",
    "\n",
    "model_path = \"/local/scratch/clmn1/octree_study/Experiments/cholec_seg_octree_2_OctreeNCA3D\"\n",
    "with open(os.path.join(model_path, \"config.dt\")) as f:\n",
    "    config = json.load(f) \n",
    "\n",
    "exp = EXP_OctreeNCA3D().createExperiment(config, detail_config={}, \n",
    "                                        dataset_class=Dataset_CholecSeg_preprocessed, dataset_args = {\n",
    "                                            'use_max_sequence_length_in_eval': False\n",
    "                                        })\n",
    "\n",
    "model: OctreeNCA3DPatch2 = exp.model\n",
    "assert isinstance(model, OctreeNCA3DPatch2)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def downscale(x: torch.Tensor, out_size):\n",
    "    x = model.align_tensor_to(x, \"BCHWD\")\n",
    "    model.remove_names(x)\n",
    "\n",
    "    out = F.interpolate(x, size=out_size)\n",
    "    out.names = ('B', 'C', 'H', 'W', 'D')\n",
    "    x.names = ('B', 'C', 'H', 'W', 'D')\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[240, 424, 1500], [120, 212, 750], [60, 106, 375], [30, 53, 188]]\n"
     ]
    }
   ],
   "source": [
    "video_tensor = torch.from_numpy(einops.rearrange(video, 'D H W C -> 1 H W D C'))\n",
    "video_tensor.names = ('B', 'H', 'W', 'D', 'C')\n",
    "computed_resolutions = model.compute_octree_res(video_tensor)\n",
    "print(computed_resolutions)\n",
    "\n",
    "seed = torch.zeros(1, *computed_resolutions[-1], model.channel_n,\n",
    "                                dtype=torch.float, device=model.device, \n",
    "                                names=('B', 'H', 'W', 'D', 'C'))\n",
    "temp = downscale(video_tensor, computed_resolutions[-1])\n",
    "temp = model.align_tensor_to(temp, \"BHWDC\")\n",
    "model.remove_names(temp)\n",
    "model.remove_names(seed)\n",
    "seed[:,:,:,:,:model.input_channels] = temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 30, 53, 188, 16])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seed.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[20, 20, 20, 40]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.inference_steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8/8 [00:30<00:00,  3.87s/it]\n"
     ]
    }
   ],
   "source": [
    "state = model.backbone_ncas[3](seed, steps=model.inference_steps[3], fire_rate=model.fire_rate)\n",
    "\n",
    "state = einops.rearrange(state, '1 H W D C -> 1 C H W D')\n",
    "state = torch.nn.Upsample(size=computed_resolutions[2], mode='nearest')(state)\n",
    "temp = F.interpolate(einops.rearrange(video_tensor, \"1 h w t c -> 1 c h w t\"), size=computed_resolutions[2])\n",
    "state[0,:model.input_channels,:,:,:] = temp[0]\n",
    "state = einops.rearrange(state, '1 C H W D -> 1 H W D C')\n",
    "state = model.backbone_ncas[2](state, steps=model.inference_steps[2], fire_rate=model.fire_rate)\n",
    "\n",
    "state = einops.rearrange(state, '1 H W D C -> 1 C H W D')\n",
    "state = torch.nn.Upsample(size=computed_resolutions[1], mode='nearest')(state)\n",
    "temp = F.interpolate(einops.rearrange(video_tensor, \"1 h w t c -> 1 c h w t\"), size=computed_resolutions[1])\n",
    "state[0,:model.input_channels,:,:,:] = temp[0]\n",
    "state = einops.rearrange(state, '1 C H W D -> 1 H W D C')\n",
    "\n",
    "new_state = torch.zeros_like(state)\n",
    "PATCH_SIZE = 300\n",
    "PADDING = model.inference_steps[1]\n",
    "for i in range(0, state.shape[3], PATCH_SIZE):\n",
    "    write_start_idx = max(i, 0)\n",
    "    write_end_idx = min(i+PATCH_SIZE, state.shape[3])\n",
    "    load_start_idx = max(i-PADDING, 0)\n",
    "    load_end_idx = min(i+PATCH_SIZE+PADDING, state.shape[3])\n",
    "    #write to new_state[:,:,:,write_start_idx:write_end_idx,:]\n",
    "    #load from state[:,:,:,load_start_idx:load_end_idx,:]\n",
    "\n",
    "    padding_start = write_start_idx - load_start_idx\n",
    "    padding_end = load_end_idx - write_end_idx\n",
    "    #print(f\"[{write_start_idx}, {write_end_idx}], [{load_start_idx}, {load_end_idx}], ({padding_start}, {padding_end})\")\n",
    "    temp_state = state[:,:,:,load_start_idx:load_end_idx,:]\n",
    "    temp = model.backbone_ncas[1](temp_state, steps=model.inference_steps[1], fire_rate=model.fire_rate)\n",
    "    #temp = torch.zeros_like(temp_state)\n",
    "\n",
    "\n",
    "    new_state[:,:,:,write_start_idx:write_end_idx,:] = temp[:,:,:,padding_start:temp.shape[3]-padding_end,:]\n",
    "\n",
    "state = new_state\n",
    "\n",
    "\n",
    "state = einops.rearrange(state, '1 H W D C -> 1 C H W D').cpu()\n",
    "state = torch.nn.Upsample(size=computed_resolutions[0], mode='nearest')(state)\n",
    "temp = F.interpolate(einops.rearrange(video_tensor, \"1 h w t c -> 1 c h w t\"), size=computed_resolutions[0])\n",
    "state[0,:model.input_channels,:,:,:] = temp[0]\n",
    "state = einops.rearrange(state, '1 C H W D -> 1 H W D C')\n",
    "\n",
    "del video_tensor, video_reader, video\n",
    "\n",
    "new_state = torch.zeros(1, *computed_resolutions[0], model.output_channels, names=('B', 'H', 'W', 'D', 'C'))\n",
    "\n",
    "PATCH_SIZE = 200\n",
    "PADDING = model.inference_steps[0]\n",
    "for i in tqdm.tqdm(range(0, state.shape[3], PATCH_SIZE)):\n",
    "    write_start_idx = max(i, 0)\n",
    "    write_end_idx = min(i+PATCH_SIZE, state.shape[3])\n",
    "    load_start_idx = max(i-PADDING, 0)\n",
    "    load_end_idx = min(i+PATCH_SIZE+PADDING, state.shape[3])\n",
    "    #write to new_state[:,:,:,write_start_idx:write_end_idx,:]\n",
    "    #load from state[:,:,:,load_start_idx:load_end_idx,:]\n",
    "\n",
    "    padding_start = write_start_idx - load_start_idx\n",
    "    padding_end = load_end_idx - write_end_idx\n",
    "    #print(f\"[{write_start_idx}, {write_end_idx}], [{load_start_idx}, {load_end_idx}], ({padding_start}, {padding_end})\")\n",
    "    temp_state = state[:,:,:,load_start_idx:load_end_idx,:]\n",
    "    temp = model.backbone_ncas[0](temp_state.to(model.device), steps=model.inference_steps[1], fire_rate=model.fire_rate).cpu()\n",
    "    #temp = torch.zeros_like(temp_state)\n",
    "\n",
    "\n",
    "    new_state[:,:,:,write_start_idx:write_end_idx,:] = temp[:,:,:,padding_start:temp.shape[3]-padding_end,model.input_channels:model.input_channels+model.output_channels]\n",
    "\n",
    "state = new_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 240, 424, 1500, 5])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state.shape # B H W D C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "segmentation = (state > 0).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "color_dict={\n",
    "    0: (252, 111, 3), \n",
    "    1: (252, 3, 227), \n",
    "    2: (205, 214, 34),\n",
    "    3: (150, 150, 150),\n",
    "    4: (0, 173, 29),\n",
    "}\n",
    "video_reader = imageio.get_reader(video_path)\n",
    "video = []\n",
    "for frame in range(int(video_reader.get_meta_data()['fps'] * num_seconds)):\n",
    "    image = video_reader.get_data(frame)\n",
    "    video.append(image[None, ...])\n",
    "\n",
    "video = np.concatenate(video, axis=0)\n",
    "video = einops.rearrange(video, 't h w c ->  h w (t c)')\n",
    "\n",
    "\n",
    "outstacks = []\n",
    "for i in range(math.ceil(video.shape[-1] / 500)):\n",
    "    outstack = cv2.resize(video[..., i*500:(i+1)*500], (424, 240))\n",
    "    outstacks.append(outstack)\n",
    "video = np.concatenate(outstacks, axis=-1)\n",
    "video = einops.rearrange(video, 'h w (t c) -> t h w c', c=3).astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "out = cv2.VideoWriter('output_0.mp4', cv2.VideoWriter_fourcc(*'mp4v'), 25, (424, 240), True)\n",
    "for i in range(segmentation.shape[3]):\n",
    "    frame_seg = segmentation[0, :, :, i]\n",
    "    frame = np.zeros((segmentation.shape[1], segmentation.shape[2], 3), dtype=np.uint8)\n",
    "    for c in range(frame.shape[-1]):\n",
    "        frame[frame_seg[..., c], :] = color_dict[c]\n",
    "    frame = 0.2 * frame + 0.8 * video[i]\n",
    "    frame = frame.astype(np.uint8)\n",
    "    out.write(frame)\n",
    "\n",
    "out.release()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
