{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nlemke/remote/miniconda3/envs/nca3/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/home/nlemke/remote/miniconda3/envs/nca3/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n"
     ]
    }
   ],
   "source": [
    "import configs, torch\n",
    "from src.models.Model_OctreeNCA_2d_patching2 import OctreeNCA2DPatch2\n",
    "import time, json, einops\n",
    "import torch.nn.functional as F\n",
    "import math, numpy as np\n",
    "\n",
    "torch.set_grad_enabled(False)\n",
    "\n",
    "study_config = {\n",
    "    'experiment.name': r'pesoS10NN',\n",
    "    'experiment.description': \"OctreeNCA2DSegmentation\",\n",
    "\n",
    "    'model.output_channels': 1,\n",
    "}\n",
    "study_config = study_config | configs.models.peso.peso_model_config\n",
    "study_config = study_config | configs.trainers.nca.nca_trainer_config\n",
    "study_config = study_config | configs.datasets.peso.peso_dataset_config\n",
    "study_config = study_config | configs.tasks.segmentation.segmentation_task_config\n",
    "study_config = study_config | configs.default.default_config\n",
    "\n",
    "study_config['experiment.logging.also_eval_on_train'] = False\n",
    "study_config['experiment.logging.evaluate_interval'] = study_config['trainer.n_epochs']+1\n",
    "study_config['experiment.task.score'] = [\"src.scores.PatchwiseDiceScore.PatchwiseDiceScore\",\n",
    "                                         \"src.scores.PatchwiseIoUScore.PatchwiseIoUScore\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "study_config['model.normalization'] = \"none\"    #\"none\"\n",
    "\n",
    "steps = 10                                      # 10\n",
    "alpha = 1.0                                     # 1.0\n",
    "study_config['model.octree.res_and_steps'] = [[[320,320], steps], [[160,160], steps], [[80,80], steps], [[40,40], steps], [[20,20], int(alpha * 20)]]\n",
    "\n",
    "\n",
    "study_config['model.channel_n'] = 16            # 16\n",
    "study_config['model.hidden_size'] = 64          # 64\n",
    "\n",
    "study_config['trainer.batch_size'] = 3          # 3\n",
    "\n",
    "dice_loss_weight = 1.0                          # 1.0\n",
    "\n",
    "\n",
    "ema_decay = 0.99                                # 0.99\n",
    "study_config['trainer.ema'] = ema_decay > 0.0\n",
    "study_config['trainer.ema.decay'] = ema_decay\n",
    "\n",
    "\n",
    "study_config['trainer.losses'] = [\"src.losses.DiceLoss.DiceLoss\", \"src.losses.BCELoss.BCELoss\"]\n",
    "study_config['trainer.losses.parameters'] = [{}, {}]\n",
    "study_config['trainer.loss_weights'] = [dice_loss_weight, 2.0-dice_loss_weight]\n",
    "#study_config['trainer.loss_weights'] = [1.5, 0.5]\n",
    "\n",
    "study_config['experiment.name'] = f\"pesofAbl_{study_config['model.normalization']}_{steps}_{alpha}_{study_config['model.channel_n']}_{study_config['trainer.batch_size']}_{dice_loss_weight}_{ema_decay}\"\n",
    "\n",
    "study_config['experiment.device'] = \"cpu\"\n",
    "\n",
    "\n",
    "\n",
    "assert study_config['model.backbone_class'] == \"BasicNCA2DFast\"\n",
    "model = OctreeNCA2DPatch2(study_config).eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_resolutions(x_shape, model):\n",
    "    upscale_factors = []\n",
    "    for i in range(len(model.octree_res)-1):\n",
    "        t = []\n",
    "        for c in range(2):\n",
    "            t.append(model.octree_res[i][c]//model.octree_res[i+1][c])\n",
    "        upscale_factors.append(t)\n",
    "\n",
    "    new_octree_res = [tuple(x_shape)]\n",
    "    for i in range(1, len(model.octree_res)):\n",
    "        downsample_factor = np.array(model.octree_res[i-1]) / np.array(model.octree_res[i])\n",
    "        new_octree_res.append([math.ceil(new_octree_res[i-1][0] / downsample_factor[0]), \n",
    "                                math.ceil(new_octree_res[i-1][1] / downsample_factor[1])])\n",
    "    return new_octree_res\n",
    "\n",
    "def remove_names(x: torch.Tensor):\n",
    "    x.names = [None] * len(x.names)\n",
    "    return x\n",
    "\n",
    "def align_tensor_to(x: torch.Tensor, target: str):\n",
    "    if isinstance(target, tuple):\n",
    "        target_str = ' '.join(target)\n",
    "    elif isinstance(target, str): \n",
    "        if max(map(len, target.split())) != 1:\n",
    "            #targets are like \"BCHW\"\n",
    "            target_str = ' '.join(target)\n",
    "        else:\n",
    "            #targets are like \"B C H W\"\n",
    "            target_str = target\n",
    "            target = target.replace(\" \", \"\")\n",
    "\n",
    "\n",
    "    pattern = f\"{' '.join(x.names)} -> {target_str}\"\n",
    "    x = remove_names(x)\n",
    "    x = einops.rearrange(x, pattern)\n",
    "    x.names = tuple(target)\n",
    "    return x\n",
    "\n",
    "def downscale(x: torch.Tensor, out_size):\n",
    "    x = align_tensor_to(x, \"BCHW\")\n",
    "    remove_names(x)\n",
    "\n",
    "    out = F.interpolate(x, size=out_size)\n",
    "    out.names = ('B', 'C', 'H', 'W')\n",
    "    x.names = ('B', 'C', 'H', 'W')\n",
    "    return out\n",
    "\n",
    "\n",
    "\n",
    "def perform_inference(slide, computed_resolutions):\n",
    "    seed = torch.zeros(1, *computed_resolutions[-1], model.channel_n,\n",
    "                                    dtype=torch.float, device=slide.device,\n",
    "                                    names=('B', 'H', 'W', 'C'))\n",
    "    temp = downscale(slide, computed_resolutions[-1])\n",
    "    temp = align_tensor_to(temp, \"BHWC\")\n",
    "    remove_names(temp)\n",
    "    remove_names(seed)\n",
    "    slide = align_tensor_to(slide, \"BHWC\")\n",
    "    remove_names(slide)\n",
    "    seed[:,:,:,:model.input_channels] = temp\n",
    "\n",
    "    state = model.backbone_ncas[4](seed, steps=model.inference_steps[4], fire_rate=model.fire_rate)\n",
    "\n",
    "    state = einops.rearrange(state, \"B H W C -> B C H W\")\n",
    "    state = torch.nn.Upsample(size=computed_resolutions[3], mode='nearest')(state)\n",
    "    temp = F.interpolate(einops.rearrange(slide, \"B H W C -> B C H W\"), size=computed_resolutions[3])\n",
    "    state[0,:model.input_channels,:,:] = temp[0]\n",
    "    state = einops.rearrange(state, \"B C H W -> B H W C\")\n",
    "    state = model.backbone_ncas[3](state, steps=model.inference_steps[3], fire_rate=model.fire_rate)\n",
    "\n",
    "\n",
    "    state = einops.rearrange(state, \"B H W C -> B C H W\")\n",
    "    state = torch.nn.Upsample(size=computed_resolutions[2], mode='nearest')(state)\n",
    "    temp = F.interpolate(einops.rearrange(slide, \"B H W C -> B C H W\"), size=computed_resolutions[2])\n",
    "    state[0,:model.input_channels,:,:] = temp[0]\n",
    "    state = einops.rearrange(state, \"B C H W -> B H W C\")\n",
    "    state = model.backbone_ncas[2](state, steps=model.inference_steps[3], fire_rate=model.fire_rate)\n",
    "\n",
    "\n",
    "    state = einops.rearrange(state, \"B H W C -> B C H W\")\n",
    "    state = torch.nn.Upsample(size=computed_resolutions[2], mode='nearest')(state)\n",
    "    temp = F.interpolate(einops.rearrange(slide, \"B H W C -> B C H W\"), size=computed_resolutions[2])\n",
    "    state[0,:model.input_channels,:,:] = temp[0]\n",
    "    state = einops.rearrange(state, \"B C H W -> B H W C\")\n",
    "    state = model.backbone_ncas[2](state, steps=model.inference_steps[2], fire_rate=model.fire_rate)\n",
    "\n",
    "\n",
    "    state = einops.rearrange(state, \"B H W C -> B C H W\")\n",
    "    state = torch.nn.Upsample(size=computed_resolutions[1], mode='nearest')(state)\n",
    "    temp = F.interpolate(einops.rearrange(slide, \"B H W C -> B C H W\"), size=computed_resolutions[1])\n",
    "    state[0,:model.input_channels,:,:] = temp[0]\n",
    "    state = einops.rearrange(state, \"B C H W -> B H W C\")\n",
    "    state = model.backbone_ncas[1](state, steps=model.inference_steps[1], fire_rate=model.fire_rate)\n",
    "\n",
    "\n",
    "    state = einops.rearrange(state, \"B H W C -> B C H W\")\n",
    "    state = torch.nn.Upsample(size=computed_resolutions[0], mode='nearest')(state)\n",
    "    temp = F.interpolate(einops.rearrange(slide, \"B H W C -> B C H W\"), size=computed_resolutions[0])\n",
    "    state[0,:model.input_channels,:,:] = temp[0]\n",
    "    state = einops.rearrange(state, \"B C H W -> B H W C\")\n",
    "    state = model.backbone_ncas[0](state, steps=model.inference_steps[0], fire_rate=model.fire_rate)\n",
    "    return state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "STAGE:2024-10-26 00:04:39 1508877:1508877 ActivityProfilerController.cpp:314] Completed Stage: Warm Up\n",
      "STAGE:2024-10-26 00:04:43 1508877:1508877 ActivityProfilerController.cpp:320] Completed Stage: Collection\n",
      "STAGE:2024-10-26 00:04:43 1508877:1508877 ActivityProfilerController.cpp:324] Completed Stage: Post Processing\n"
     ]
    }
   ],
   "source": [
    "def perform_inference_and_measure_time(img_dim):\n",
    "    input_img = torch.rand(1, 3, img_dim, img_dim, names=('B', 'C', 'H', 'W'))  #this must be BCHW\n",
    "    computed_resolutions = compute_resolutions(input_img.shape[2:], model)\n",
    "    start = time.time()\n",
    "    out = perform_inference(input_img, computed_resolutions)\n",
    "    end = time.time()\n",
    "    assert out.shape[1:3] == (img_dim, img_dim)\n",
    "    return end-start\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "STAGE:2024-10-26 00:25:54 1508877:1508877 ActivityProfilerController.cpp:314] Completed Stage: Warm Up\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                        Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg    # of Calls  \n",
      "----------------------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "             model_inference        27.31%        2.732s       100.00%       10.003s       10.003s             1  \n",
      "                aten::conv2d         0.03%       2.980ms        27.22%        2.723s      12.966ms           210  \n",
      "           aten::convolution         0.02%       2.128ms        27.21%        2.722s      12.961ms           210  \n",
      "          aten::_convolution         0.02%       2.240ms        27.19%        2.720s      12.953ms           210  \n",
      "    aten::mkldnn_convolution        27.15%        2.715s        27.17%        2.718s      12.942ms           210  \n",
      "                   aten::cat        17.76%        1.776s        17.78%        1.779s      12.704ms           140  \n",
      "                  aten::relu         0.01%     674.000us        14.78%        1.478s      21.118ms            70  \n",
      "             aten::clamp_min        14.77%        1.478s        14.77%        1.478s      21.109ms            70  \n",
      "                   aten::add         4.50%     449.925ms         4.50%     449.925ms       6.428ms            70  \n",
      "                   aten::pad         0.01%     510.000us         3.80%     380.039ms       5.429ms            70  \n",
      "      aten::reflection_pad2d         3.79%     378.937ms         3.79%     379.595ms       5.423ms            70  \n",
      "                   aten::mul         3.14%     314.386ms         3.14%     314.386ms       4.491ms            70  \n",
      "                  aten::rand         0.00%      29.000us         0.66%      66.016ms      66.016ms             1  \n",
      "              aten::uniform_         0.66%      65.925ms         0.66%      65.925ms      65.925ms             1  \n",
      "    aten::upsample_nearest2d         0.42%      42.303ms         0.42%      42.410ms       3.855ms            11  \n",
      "                 aten::copy_         0.29%      28.791ms         0.29%      28.791ms       4.798ms             6  \n",
      "            aten::bernoulli_         0.04%       3.997ms         0.04%       4.051ms      57.871us            70  \n",
      "                 aten::zeros         0.01%     508.000us         0.03%       2.636ms      37.127us            71  \n",
      "                 aten::slice         0.02%       2.147ms         0.03%       2.615ms       6.864us           381  \n",
      "               aten::resize_         0.02%       2.304ms         0.02%       2.304ms       8.229us           280  \n",
      "                aten::narrow         0.01%     979.000us         0.02%       2.249ms      10.710us           210  \n",
      "                 aten::fill_         0.02%       1.532ms         0.02%       1.532ms      66.609us            23  \n",
      "                 aten::zero_         0.00%     131.000us         0.01%       1.465ms      20.929us            70  \n",
      "                 aten::empty         0.01%       1.322ms         0.01%       1.322ms       2.964us           446  \n",
      "            aten::as_strided         0.01%     503.000us         0.01%     503.000us       1.181us           426  \n",
      "               aten::permute         0.00%     163.000us         0.00%     183.000us       6.310us            29  \n",
      "                  aten::ones         0.00%      46.000us         0.00%      84.000us       7.000us            12  \n",
      "                aten::select         0.00%      71.000us         0.00%      77.000us       7.700us            10  \n",
      "                    aten::to         0.00%       0.000us         0.00%       0.000us       0.000us            70  \n",
      "----------------------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "Self CPU time total: 10.003s\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "STAGE:2024-10-26 00:26:04 1508877:1508877 ActivityProfilerController.cpp:320] Completed Stage: Collection\n",
      "STAGE:2024-10-26 00:26:04 1508877:1508877 ActivityProfilerController.cpp:324] Completed Stage: Post Processing\n"
     ]
    }
   ],
   "source": [
    "from torch.profiler import profile, record_function, ProfilerActivity\n",
    "with profile(activities=[ProfilerActivity.CPU], record_shapes=True) as prof:\n",
    "    with record_function(\"model_inference\"):\n",
    "        perform_inference_and_measure_time(2576)#320*5\n",
    "print(prof.key_averages().table(sort_by=\"cpu_time_total\", row_limit=500))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "320\n",
      "run 0\n",
      "run 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1508877/2040333383.py:2: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at ../c10/core/TensorImpl.h:1921.)\n",
      "  input_img = torch.rand(1, 3, img_dim, img_dim, names=('B', 'C', 'H', 'W'))  #this must be BCHW\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "run 2\n",
      "640\n",
      "run 0\n",
      "run 1\n",
      "run 2\n",
      "960\n",
      "run 0\n",
      "run 1\n",
      "run 2\n",
      "1280\n",
      "run 0\n",
      "run 1\n",
      "run 2\n",
      "1600\n",
      "run 0\n",
      "run 1\n",
      "run 2\n"
     ]
    }
   ],
   "source": [
    "results = {}\n",
    "for img_dim in [320, 320*2, 320*3, 320*4, 320*5]:\n",
    "    print(img_dim)\n",
    "    timings = []\n",
    "    for i in range(3):\n",
    "        print(\"run\", i)\n",
    "        timings.append(perform_inference_and_measure_time(img_dim))\n",
    "\n",
    "    results[img_dim] = timings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"john_timing_results_oct_pi.json\", \"w\") as f:\n",
    "    json.dump(results, f, indent=4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
